{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Prominence Detection\n",
    "Speech Prominence Detection is the process of identifying the most important or prominent parts of speech in an audio signal. Prominence refers to the degree of emphasis or attention that a particular word or phrase receives in spoken language, which is often conveyed through variations in pitch, loudness, and timing. Speech Prominence Detection is an essential task in speech processing and natural language understanding, with a wide range of applications including speech recognition, sentiment analysis, and language translation. The goal of this project is to develop a machine learning model that can accurately identify the prominent parts of speech in a given audio signal. This project will involve feature extraction, model training, and evaluation, with the aim of achieving high accuracy and generalizability on a diverse range of speech datasets. The results of this project could have significant implications for improving speech recognition and understanding systems in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 2\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m \u001b[39mimport\u001b[39;00m medfilt\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m clear_output\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\signal\\__init__.py:309\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m=======================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mSignal processing (:mod:`scipy.signal`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _sigtools, windows\n\u001b[0;32m    310\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_waveforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    311\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_max_len_seq\u001b[39;00m \u001b[39mimport\u001b[39;00m max_len_seq\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import collections\n",
    "import scipy\n",
    "from scipy.signal import medfilt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myfunctions import spectral_selection,  temporal_corr, get_labels_seq2seq\n",
    "from myfunctions import spectral_corr, smooth, vocoder_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "ger_test_dir = os.path.join(data_dir, \"GER/test/\")\n",
    "ger_train_dir = os.path.join(data_dir, \"GER/train/\")\n",
    "ita_test_dir = os.path.join(data_dir, \"ITA/test/\")\n",
    "ita_train_dir = os.path.join(data_dir, \"ITA/train/\")\n",
    "\n",
    "phn_dir = os.path.join(data_dir, \"fisher-2000_FA_GT_ESTphnTrans_estStress/lab/txt/phn/\")\n",
    "dict_name = \"nativeEnglishDict_gt100_manoj.syl\"\n",
    "stressLabelspath = data_dir + \"FA_htkCorrectedLabWithFullAudio\" + \"/lab/mat/sylStress/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chech if the directories exist\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Data directory does not exist\")\n",
    "if not os.path.exists(ger_train_dir):\n",
    "    print(\"German Train directory does not exist\")\n",
    "if not os.path.exists(ita_train_dir):\n",
    "    print(\"Italian Train directory does not exist\")\n",
    "if not os.path.exists(phn_dir):\n",
    "    print(\"Phoneme directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_test_files = os.listdir(ger_test_dir)\n",
    "ger_train_files = os.listdir(ger_train_dir)\n",
    "ita_test_files = os.listdir(ita_test_dir)\n",
    "ita_train_files = os.listdir(ita_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features\n",
    "twin = 5\n",
    "t_sigma = 1.4\n",
    "swin = 7\n",
    "s_sigma = 1.5\n",
    "mwin = 13\n",
    "max_threshold = 25\n",
    "\n",
    "vwlSB_num = 4\n",
    "vowelSB = [1, 2, 4, 5, 6, 7, 8, 13, 14, 15, 16, 17]\n",
    "sylSB_num = 5\n",
    "sylSB = [1, 2, 3, 4, 5, 6, 13, 14, 15, 16, 17, 18]\n",
    "\n",
    "startWordFrame_all = []\n",
    "spurtStartFrame_all = []\n",
    "spurtEndFrame_all = []\n",
    "vowelStartFrame_all = []\n",
    "vowelEndFrame_all = []\n",
    "eng_full_all = []\n",
    "spurtStress_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_array(phn_file):\n",
    "    data_array = []\n",
    "    try:\n",
    "        fid = open(phn_file, 'r')\n",
    "        data_array = np.loadtxt(fid, dtype={'names': ('a', 'b', 'c'), 'formats': ('f4', 'f4', 'S16')})\n",
    "        fid.close\n",
    "    except:\n",
    "        print('File does not exist')\n",
    "        return\n",
    "\n",
    "    ghastly = []\n",
    "    for i in range(len(data_array)):\n",
    "        tuple_list = list(data_array[i])\n",
    "        tuple_list[2] = tuple_list[2].decode()\n",
    "        ghastly.append((tuple_list[0], tuple_list[1], tuple_list[2]))\n",
    "    return np.array(ghastly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_data(data_array):\n",
    "    phnTimes1 = [row[0] for row in data_array]\n",
    "    phnTimes1 = np.array([phnTimes1]).T\n",
    "\n",
    "    phnTimes2 = [row[1] for row in data_array]\n",
    "    phnTimes2 = np.array([phnTimes2]).T\n",
    "\n",
    "    phnTimes = np.hstack((phnTimes1, phnTimes2))\n",
    "    phones = [row[2] for row in data_array]\n",
    "    phones = np.array([phones])\n",
    "\n",
    "    # Made them lowercase since the syl dictionary is in lowercase\n",
    "    for i in range(0, len(phones[0])):\n",
    "        phones[0][i] = phones[0][i].lower()\n",
    "        \n",
    "    origPhones = phones\n",
    "    index = np.argwhere(origPhones[0] == 'sil')\n",
    "    phones = phones[phones != 'sil']\n",
    "    phones = np.array([phones])\n",
    "    phones = phones.reshape(1, -1)\n",
    "\n",
    "    phnTimes2 = np.delete(phnTimes2, index, axis=0)\n",
    "    phnTimes = np.delete(phnTimes, index, axis=0)\n",
    "    \n",
    "    return phones, phnTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting vowel data\n",
    "def get_vowel_data(data_array):\n",
    "    # VOWEL LIST\n",
    "    vowelList = ['aa', 'ae', 'ah', 'ao', 'aw', 'ay', 'eh', 'er', 'ey', 'ih', 'iy', 'ow', 'oy', 'uh', 'uw']\n",
    "    vowel_start_time = []\n",
    "    vowel_end_time = []\n",
    "    vowel = []\n",
    "\n",
    "    for i in range(0, len(data_array)):\n",
    "        if data_array[i][2].lower() in vowelList:\n",
    "            vowel_start_time.append(data_array[i][0])\n",
    "            vowel_end_time.append(data_array[i][1])\n",
    "            vowel.append(data_array[i][2].lower())\n",
    "            \n",
    "    vowel_start_time = np.array([vowel_start_time])\n",
    "    vowel_end_time = np.array([vowel_end_time])\n",
    "    vowel = np.array([vowel])\n",
    "    return vowel, vowel_start_time, vowel_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(file_name):\n",
    "    # Define the path to the transcript file\n",
    "    trans_path = data_dir + \"ISLEtrans.txt\"\n",
    "\n",
    "    # Read the contents of the transcript file\n",
    "    with open(trans_path, 'r') as trans_file:\n",
    "        trans_contents = trans_file.read()\n",
    "\n",
    "    # Extract the lines containing the specified filename from the transcript\n",
    "    lines = [line for line in trans_contents.split('\\n') if file_name in line]\n",
    "\n",
    "    # Extract the words from the lines and clean them up\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        _, word_list = line.split(' ', 1)\n",
    "        words.extend(re.findall(r'\\b\\w+\\b', word_list))\n",
    "    words = [word.lower() for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_syls(words):\n",
    "    d = collections.defaultdict(list)\n",
    "    with open(data_dir + dict_name, 'r') as f:\n",
    "        for line in f:\n",
    "            key = line.split()[0]\n",
    "            val = line.split('=')[1].strip()\n",
    "            d[key].append(val)\n",
    "\n",
    "    word_syls = []\n",
    "    for i in range(len(words)):\n",
    "        curr_word_syls = []\n",
    "        if words[i] in d:\n",
    "            curr_word_syls = d[words[i]]\n",
    "        word_syls.append(curr_word_syls)\n",
    "    return word_syls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_indices(words, word_syls, phones):\n",
    "    newSuccessInds_all = []\n",
    "    newSuccessInds_all2 = []\n",
    "\n",
    "    prevSuccessInds_all = []\n",
    "    prevSuccessInds_all.append(0)\n",
    "\n",
    "    # I said got not goat\n",
    "    for iterWord in range(0, len(words)):\n",
    "        currWordSyls = word_syls[iterWord]\n",
    "        countSuccess = 1\n",
    "\n",
    "        for iterPrev in range(0, len(prevSuccessInds_all)):\n",
    "            prevWordSyls = \"\"\n",
    "            if prevSuccessInds_all[iterPrev] == 0:\n",
    "                currPrevSylInds = []\n",
    "            else:\n",
    "                currPrevSylInds = prevSuccessInds_all[iterPrev]\n",
    "                for iterPrevSyls in range(0, len(currPrevSylInds)):\n",
    "                    temp = word_syls[iterPrevSyls]\n",
    "                    prevWordSyls = prevWordSyls + \\\n",
    "                        temp[currPrevSylInds[iterPrevSyls]]+\" \"\n",
    "\n",
    "            # iterating through the syllables of the current word\n",
    "            for iterCurr in range(0, len(currWordSyls)):\n",
    "                currTestWordSyls = prevWordSyls + currWordSyls[iterCurr]\n",
    "                temp2 = currTestWordSyls.replace(' . ', ' ')\n",
    "                \n",
    "                \n",
    "                inds = [m.start() for m in re.finditer(' ', temp2)]\n",
    "                if len(inds) == 0:\n",
    "                    inds = [len(temp2)]\n",
    "\n",
    "                count = 1\n",
    "                temp = []\n",
    "\n",
    "                for iterTemp in range(len(inds)):\n",
    "                    if iterTemp == 0:\n",
    "                        temp1 = temp2[0:inds[iterTemp]]\n",
    "                        # print(temp2 + \"\\t\\t| \" + temp1)\n",
    "                    else:\n",
    "                        temp1 = temp2[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "                    if not ((np.unique(temp1) == ' ').any() or (len(temp1) == 0)):\n",
    "                        temp.append(temp1)\n",
    "                        count += 1\n",
    "                        \n",
    "                if iterTemp == len(inds) - 1 and len(inds) < len(currTestWordSyls):\n",
    "                    temp1 = temp2[inds[iterTemp]+1:len(temp2)]\n",
    "                    if not ((len(temp1) == 0) or (np.unique(temp1) == ' ').any()):\n",
    "                        temp.append(temp1)\n",
    "                        count = count+1\n",
    "\n",
    "                if iterWord + 1 == len(words):\n",
    "                    currPhones = phones[0, 0:len(phones[0])]\n",
    "                else:\n",
    "                    currPhones = phones[0][0:len(temp)]\n",
    "\n",
    "                    \n",
    "                flag = 1\n",
    "                for iterFlag in range(0, len(currPhones), 1):\n",
    "                    if len(currPhones) != len(temp):\n",
    "                        flag = 0\n",
    "                    else:\n",
    "                        if currPhones[iterFlag] != temp[iterFlag]:\n",
    "                            flag = 0\n",
    "                if flag == 1:\n",
    "                    if not currPrevSylInds == []:\n",
    "                        for i in range(0, len(currPrevSylInds)):\n",
    "                            #                            print('line 122::::::yes')\n",
    "                            newSuccessInds_all.append(currPrevSylInds[i])\n",
    "                    newSuccessInds_all.append(iterCurr)\n",
    "                    newSuccessInds_all2.append(newSuccessInds_all)\n",
    "                    newSuccessInds_all = []\n",
    "                    countSuccess = countSuccess+1\n",
    "                    \n",
    "        prevSuccessInds_all = newSuccessInds_all2\n",
    "        newSuccessInds_all2 = []\n",
    "    if len(prevSuccessInds_all) == 0:\n",
    "        return None, None\n",
    "    return prevSuccessInds_all[0], currTestWordSyls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syls_count(path_indices, currTestWordSyls, words, word_syls):\n",
    "    sylCount = 1\n",
    "    phnCount = 1\n",
    "    spurtSyl = []  # spurtSylTimes= np.zeros((len(phnTimes),2))\n",
    "\n",
    "    syls_word = np.zeros((1, len(path_indices)))\n",
    "    spurtWordTimes = np.zeros((len(path_indices), 2))\n",
    "\n",
    "    for iterPath in range(0, len(path_indices)):\n",
    "        # current word and syllables\n",
    "        currWord = words[iterPath]\n",
    "        currWordSyls = word_syls[iterPath]\n",
    "\n",
    "        currSyl = currWordSyls[path_indices[iterPath]]\n",
    "        currSyl = currSyl.replace(' . ', '.')\n",
    "        # print(currSyl)\n",
    "        inds = [m.start() for m in re.finditer('\\.', currSyl)]\n",
    "\n",
    "        if len(inds) == 0:\n",
    "            inds = [len(currSyl)]\n",
    "\n",
    "        count = 0\n",
    "        for iterTemp in range(0, len(inds)):\n",
    "            if iterTemp == 0:\n",
    "                temp1 = currSyl[0:inds[iterTemp]]\n",
    "            else:\n",
    "                temp1 = currSyl[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                spurtSyl.append(temp1)\n",
    "                sylCount = sylCount + 1\n",
    "                count = count + 1\n",
    "                \n",
    "        if iterTemp is len(inds)-1 and len(inds) < len(currTestWordSyls):\n",
    "            temp1 = currSyl[inds[iterTemp]+1:len(currSyl)]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                spurtSyl.append(temp1)\n",
    "                sylCount = sylCount + 1\n",
    "                count = count + 1\n",
    "        syls_word[0][iterPath] = count\n",
    "\n",
    "    return syls_word, spurtSyl, spurtWordTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurts(spurtSyl, currTestWordSyls, phnTimes):\n",
    "    phnCount = 1\n",
    "    spurtSylTimes = np.zeros((len(spurtSyl), 2))\n",
    "\n",
    "    for iterSyl in range(0, len(spurtSyl)):\n",
    "        temp2 = spurtSyl[iterSyl]\n",
    "        inds = [m.start() for m in re.finditer(' ', temp2)]\n",
    "        if len(inds) == 0:\n",
    "            inds = [len(temp2)]\n",
    "        count = 1\n",
    "        temp = []\n",
    "        for iterTemp in range(0, len(inds)):\n",
    "            if iterTemp == 0:\n",
    "                temp1 = temp2[0:inds[iterTemp]]\n",
    "            else:\n",
    "                temp1 = temp2[inds[iterTemp-1]+1:inds[iterTemp]]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                temp.append(temp1)\n",
    "                count = count+1\n",
    "        if iterTemp == len(inds)-1 and len(inds) < len(currTestWordSyls):\n",
    "            temp1 = temp2[inds[iterTemp]+1:len(temp2)]\n",
    "            if not (temp1 == ' ' or len(temp1) == 0):\n",
    "                temp.append(temp1)\n",
    "                count = count+1\n",
    "\n",
    "        nPhns_syl = len(temp)\n",
    "        spurtSylTimes[iterSyl, 0] = phnTimes[phnCount-1, 0]\n",
    "        phnCount = phnCount + nPhns_syl\n",
    "        spurtSylTimes[iterSyl, 1] = phnTimes[phnCount-1-1, 1]\n",
    "    return spurtSylTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spurt_word_times(path_indices, syls_word, spurtSylTimes):\n",
    "    spurtWordTimes = np.zeros((len(path_indices), 2))\n",
    "    sylIdx = 1\n",
    "    print(\"this = \", len(syls_word[0]))\n",
    "\n",
    "    for iterWordTimes in range(0, len(syls_word[0])):\n",
    "        spurtWordTimes[iterWordTimes, 0] = spurtSylTimes[sylIdx-1, 0]\n",
    "        sylIdx = sylIdx + syls_word[0][iterWordTimes].astype(int)\n",
    "        spurtWordTimes[iterWordTimes, 1] = spurtSylTimes[sylIdx-1-1, 1]\n",
    "    length_spurtWordTimes = iterWordTimes + 1\n",
    "    return spurtWordTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_boundaries(spurtWordTimes, words, spurtSylTimes):\n",
    "    # Processing word boundary file\n",
    "    # FILE READ DELETED HERE\n",
    "    a = spurtWordTimes\n",
    "    b = words\n",
    "    if (len(a) is not len(b)):\n",
    "        print(\"error\")\n",
    "    wordData = np.hstack((a, np.array([b], dtype='S32').T))\n",
    "\n",
    "    # Extract first coloumn of wordData\n",
    "    startWordTime = [row[0] for row in wordData]\n",
    "    endWordTime = [row[1] for row in wordData]\n",
    "\n",
    "    startWordFrame = np.round((np.subtract(np.array(startWordTime, dtype='float'), spurtSylTimes[0][0].astype(float))*100))\n",
    "    endWordFrame = np.round((np.subtract(np.array(endWordTime, dtype='float'), spurtSylTimes[0][0].astype(float))*100) + 1)\n",
    "    startWordFrame = np.append(startWordFrame, endWordFrame[-1])\n",
    "\n",
    "    return startWordFrame, endWordFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowelStartTime, startWordFrame):\n",
    "    # TCSSBC computation\n",
    "    if len(sylSB) > sylSB_num:\n",
    "        eng = spectral_selection(\n",
    "            eng_full[np.subtract(sylSB, 1), :], sylSB_num)\n",
    "    else:\n",
    "        eng = eng_full[sylSB, :]\n",
    "\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)\n",
    "    s_cor = spectral_corr(t_cor)\n",
    "    sylTCSSBC = smooth(s_cor, swin, s_sigma)\n",
    "    sylTCSSBC = np.array([sylTCSSBC])\n",
    "\n",
    "    start_idx = np.round(spurtStartTime[0]*100).astype(int)\n",
    "    sylTCSSBC = np.array([sylTCSSBC[0][start_idx:-1]])\n",
    "\n",
    "    sylTCSSBC = np.divide(sylTCSSBC, max(sylTCSSBC[0]))\n",
    "\n",
    "    if len(vowelSB) > vwlSB_num:\n",
    "        eng = spectral_selection(\n",
    "            eng_full[np.subtract(vowelSB, 1), :], vwlSB_num)\n",
    "    else:\n",
    "        eng = eng_full[vowelSB, :]\n",
    "    t_cor = temporal_corr(eng, twin, t_sigma)\n",
    "    s_cor = spectral_corr(t_cor)\n",
    "    vwlTCSSBC = smooth(s_cor, swin, s_sigma)\n",
    "\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC])\n",
    "\n",
    "    # Modify TCSSBC contour by clipping from the vowel start\n",
    "    start_idx = np.round(vowelStartTime[0][0]*100).astype(int)\n",
    "    vwlTCSSBC = np.array([vwlTCSSBC[0][start_idx:-1]])\n",
    "\n",
    "    vwlTCSSBC = np.divide(vwlTCSSBC, max(vwlTCSSBC[0]))\n",
    "\n",
    "    # Compute silence statistics\n",
    "    # Preprocessing of the data\n",
    "    word_duration = np.zeros((1, len(startWordFrame) - 1))\n",
    "    word_Sylsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "    word_Vwlsum = np.zeros((1, len(startWordFrame) - 1))\n",
    "\n",
    "    for j in range(0, len(startWordFrame) - 1):\n",
    "        temp_start = startWordFrame[j].astype(int)\n",
    "        temp_end = startWordFrame[j + 1].astype(int) - 1\n",
    "        # jhansi\n",
    "        if (temp_end >= sylTCSSBC.shape[1]):\n",
    "            temp_end1 = sylTCSSBC.shape[1]-1\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end1)] = medfilt(\n",
    "                sylTCSSBC[0, np.arange(temp_start, temp_end1)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end1] = sylTCSSBC[0, temp_end1 - 1]\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end1)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)\n",
    "        else:\n",
    "            sylTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "                sylTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "            sylTCSSBC[0, temp_start] = sylTCSSBC[0, temp_start+1]\n",
    "            sylTCSSBC[0, temp_end] = sylTCSSBC[0, temp_end - 1]\n",
    "            tempArr = sylTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "            word_Sylsum[0, j] = tempArr.sum(axis=0)\n",
    "        if (temp_end >= vwlTCSSBC.shape[1]):\n",
    "            temp_end = vwlTCSSBC.shape[1]-1\n",
    "\n",
    "        #    temp_end = np.min([temp_end,len(vwlTCSSBC)])\n",
    "        vwlTCSSBC[0, np.arange(temp_start, temp_end)] = medfilt(\n",
    "            vwlTCSSBC[0, np.arange(temp_start, temp_end)], 3)\n",
    "        vwlTCSSBC[0, temp_start] = vwlTCSSBC[0, temp_start+1]\n",
    "        vwlTCSSBC[0, temp_end] = vwlTCSSBC[0, temp_end - 1]\n",
    "\n",
    "        word_duration[0, j] = temp_end - temp_start + 1\n",
    "\n",
    "        tempArr = vwlTCSSBC[0, np.arange(temp_start, temp_end)]\n",
    "        word_Vwlsum[0, j] = tempArr.sum(axis=0)\n",
    "\n",
    "    sylTCSSBC[np.isnan(sylTCSSBC)] = 0   # Feature vector 1\n",
    "    vwlTCSSBC[np.isnan(vwlTCSSBC)] = 0   # Feature vector 2\n",
    "    return sylTCSSBC[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_feature_contour(sylTCSSBC, spurtStartFrame, spurtEndFrame):\n",
    "    # Chunking the feature contour\n",
    "    sylTCSSBC_chunk = []\n",
    "    for i in range(0, len(spurtStartFrame)):\n",
    "        sylTCSSBC_chunk.append(sylTCSSBC[int(spurtStartFrame[i]):int(spurtEndFrame[i])])\n",
    "    return np.array(sylTCSSBC_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_contour(wav_file, test_data):\n",
    "    file_name = wav_file[:-4]\n",
    "    phn_file = phn_dir + file_name + \".txt\"\n",
    "    mat_file = stressLabelspath + file_name + \".mat\"\n",
    "\n",
    "    \n",
    "\n",
    "    if not os.path.exists(phn_file):\n",
    "        print(\"phn file doesn't exist\")\n",
    "        return None, None, False\n",
    "    \n",
    "    if not os.path.exists(mat_file):\n",
    "        print(\"mat file doesn't exist\")\n",
    "        return None, None, False\n",
    "\n",
    "    data_array = get_data_array(phn_file)\n",
    "    phones, phn_times = get_phone_data(data_array)\n",
    "    vowel, vowel_start_time, vowel_end_time = get_vowel_data(data_array)\n",
    "    words = get_words(file_name)\n",
    "    word_syls = get_word_syls(words)\n",
    "    \n",
    "    \n",
    "    path_indices, currTestWordSyls = get_path_indices(words, word_syls, phones)\n",
    "\n",
    "    if path_indices == None:\n",
    "        return None, None, False\n",
    "    syls_word, spurtSyl, spurtWordTimes = get_syls_count(path_indices, currTestWordSyls, words, word_syls)\n",
    "    spurtSylTimes = get_spurts(spurtSyl, currTestWordSyls, phn_times)\n",
    "    syls_word = syls_word.astype('i')\n",
    "\n",
    "    poly = []\n",
    "    for i in range(len(syls_word[0])):\n",
    "        if syls_word[0][i] > 1:\n",
    "            lst = [True] * syls_word[0][i]\n",
    "            poly.extend(lst)\n",
    "        else:\n",
    "            poly.append(False)\n",
    "\n",
    "    num_poly = sum(poly)\n",
    "\n",
    "    if num_poly == 0:\n",
    "        return None, None, False\n",
    "\n",
    "    # path_indices = [path_indices[i] for i in poly]\n",
    "    # syls_word = [syls_word[i] for i in poly]\n",
    "    # spurtSylTimes = [spurtSylTimes[i] for i in poly]\n",
    "    # words = [words[i] for i in poly]\n",
    "    syls_word = [[syls_word[0][i] for i in range(len(syls_word[0])) if poly[i]]]\n",
    "    path_indices = [path_indices[i] for i in range(len(path_indices)) if poly[i]]\n",
    "    spurtSylTimes = [spurtSylTimes[i] for i in range(len(spurtSylTimes)) if poly[i]]\n",
    "    spurtSylTimes = [list(i) for i in zip(*spurtSylTimes)]\n",
    "    spurtSylTimes = np.array(spurtSylTimes)\n",
    "    words = [words[i] for i in range(len(words)) if poly[i]]\n",
    "    vowel_start_time = np.array([[vowel_start_time[0][i] for i in range(len(vowel_start_time[0])) if poly[i]]])\n",
    "    vowel_end_time = np.array([[vowel_end_time[0][i] for i in range(len(vowel_end_time[0])) if poly[i]]])\n",
    "    print(syls_word)\n",
    "    print(words)\n",
    "\n",
    "\n",
    "    spurtWordTimes = get_spurt_word_times(path_indices, syls_word, spurtSylTimes)\n",
    "\n",
    "    # Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\n",
    "    file_dir = ger_test_dir if test_data else ger_train_dir\n",
    "    eng_full, xx = vocoder_func(file_dir + wav_file)\n",
    "    eng_full = eng_full.conj().transpose()\n",
    "\n",
    "\n",
    "    startWordFrame, endWordFrame = process_word_boundaries(spurtWordTimes, words, spurtSylTimes)\n",
    "\n",
    "    # Processing of stress and syllable boundary file\n",
    "    spurtSylTime = spurtSylTimes\n",
    "    spurtStartTime = spurtSylTime[:][0]\n",
    "    spurtEndTime = spurtSylTime[:][1]\n",
    "    spurtStartFrame = np.round((spurtStartTime - spurtStartTime[0]) * 100)\n",
    "    spurtEndFrame = np.round((spurtEndTime - spurtStartTime[0]) * 100)\n",
    "\n",
    "    # Processing of Vowel boundary file\n",
    "    vowel_start_time = vowel_start_time.astype(float)\n",
    "    vowel_end_time = vowel_end_time.astype(float)\n",
    "\n",
    "    vowelStartFrame = np.round(vowel_start_time*100 - spurtStartTime[0]*100)\n",
    "    vowelEndFrame = np.round(vowel_end_time*100 - spurtStartTime[0]*100)\n",
    "\n",
    "    sylTCSSBC = get_sylTCSSBC(sylSB, eng_full, sylSB_num, twin, t_sigma, swin, s_sigma, spurtStartTime, vowel_start_time, startWordFrame)\n",
    "    sylTCSSBC_chunk = chunk_feature_contour(sylTCSSBC, spurtStartFrame, spurtEndFrame)\n",
    "\n",
    "    # if test_data:\n",
    "        # return sylTCSSBC_chunk, None\n",
    "\n",
    "\n",
    "    # extract label\n",
    "    mat = scipy.io.loadmat(stressLabelspath + file_name + '.mat')\n",
    "    lab = mat['spurtStress']\n",
    "    lab_list = lab.tolist()\n",
    "    labels = get_labels_seq2seq(lab_list)  # Labels\n",
    "\n",
    "    if len(sylTCSSBC_chunk) != len(labels):\n",
    "        return None, None, False\n",
    "\n",
    "    return sylTCSSBC_chunk, labels, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_train_files_subset = ger_train_files[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]]\n",
      "['centre']\n",
      "this =  1\n"
     ]
    }
   ],
   "source": [
    "all_contours = []\n",
    "all_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_train_files_subset):\n",
    "    contours, labels, valid = feature_contour(file, False)\n",
    "    if not valid: continue\n",
    "\n",
    "    all_contours.extend(contours)\n",
    "    all_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]]\n",
      "['centre']\n",
      "this =  1\n",
      "[[3]]\n",
      "['tomorrow']\n",
      "this =  1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 23\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ger_train_files):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     contours, labels, success \u001b[39m=\u001b[39m feature_contour(file, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m contours \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 23\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(syls_word)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(words)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m spurtWordTimes \u001b[39m=\u001b[39m get_spurt_word_times(path_indices, syls_word, spurtSylTimes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m file_dir \u001b[39m=\u001b[39m ger_test_dir \u001b[39mif\u001b[39;00m test_data \u001b[39melse\u001b[39;00m ger_train_dir\n",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 23\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     spurtWordTimes[iterWordTimes, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m spurtSylTimes[sylIdx\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     sylIdx \u001b[39m=\u001b[39m sylIdx \u001b[39m+\u001b[39m syls_word[\u001b[39m0\u001b[39m][iterWordTimes]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     spurtWordTimes[iterWordTimes, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m spurtSylTimes[sylIdx\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m length_spurtWordTimes \u001b[39m=\u001b[39m iterWordTimes \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m spurtWordTimes\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "all_chunks = []\n",
    "all_labels = []\n",
    "for i, file in enumerate(ger_train_files):\n",
    "    contours, labels, success = feature_contour(file, False)\n",
    "    if contours == None:\n",
    "        continue\n",
    "    all_chunks.extend(contours)\n",
    "    all_labels.extend(labels)   \n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_train_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_train_files)), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]]\n",
      "['centre']\n",
      "this =  1\n",
      "[[3]]\n",
      "['tomorrow']\n",
      "this =  1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 24\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ger_train_files):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     contours, labels, success \u001b[39m=\u001b[39m feature_contour(file, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m contours \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 24\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(syls_word)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(words)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m spurtWordTimes \u001b[39m=\u001b[39m get_spurt_word_times(path_indices, syls_word, spurtSylTimes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Execute the vocoder [MODIFICATION]: Get the audio file back so that it can be stored in a text file for C code.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m file_dir \u001b[39m=\u001b[39m ger_test_dir \u001b[39mif\u001b[39;00m test_data \u001b[39melse\u001b[39;00m ger_train_dir\n",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 24\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     spurtWordTimes[iterWordTimes, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m spurtSylTimes[sylIdx\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     sylIdx \u001b[39m=\u001b[39m sylIdx \u001b[39m+\u001b[39m syls_word[\u001b[39m0\u001b[39m][iterWordTimes]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     spurtWordTimes[iterWordTimes, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m spurtSylTimes[sylIdx\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m length_spurtWordTimes \u001b[39m=\u001b[39m iterWordTimes \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m spurtWordTimes\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "all_chunks = []\n",
    "all_labels = []\n",
    "start = 4\n",
    "for i, file in enumerate(ger_train_files):\n",
    "    contours, labels, success = feature_contour(file, False)\n",
    "    if contours == None:\n",
    "        continue\n",
    "    all_chunks.append(contours)\n",
    "    all_labels.append(labels)   \n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_train_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_train_files)), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\abhay\\Desktop\\Code\\Speech-Prominence-Detection\\Pranjal_Abhay\\codes\\features.ipynb Cell 25\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ita_train_files):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     chunks, labels \u001b[39m=\u001b[39m feature_contour(file, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m chunks \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abhay/Desktop/Code/Speech-Prominence-Detection/Pranjal_Abhay/codes/features.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "all_chunks = []\n",
    "all_labels = []\n",
    "for i, file in enumerate(ita_train_files):\n",
    "    chunks, labels = feature_contour(file, False)\n",
    "    if chunks == None:\n",
    "        continue\n",
    "    all_chunks.extend(chunks)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ita_train_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ita_train_files)), end=\"\\r\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': all_chunks, 'labels': all_labels})\n",
    "df.to_pickle('../saved/ger_train_poly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.94%\rProcessed: 1767/1768\r"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "test_chunks = []\n",
    "test_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_test_files):\n",
    "    chunks, labels, success = feature_contour(file, True)\n",
    "    if not success:\n",
    "        continue\n",
    "    test_chunks.extend(chunks)\n",
    "    test_labels.extend(labels)\n",
    "\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ger_test_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ger_test_files)), end=\"\\r\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': test_chunks, 'labels': test_labels})\n",
    "df.to_pickle('../saved/ger_test_poly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.70%\rProcessed: 1685/1690\r"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "test_chunks = []\n",
    "test_labels = []\n",
    "\n",
    "for i, file in enumerate(ger_test_files):\n",
    "    chunks, labels, success = feature_contour(file, True)\n",
    "    if not success:\n",
    "        continue\n",
    "    test_chunks.extend(chunks)\n",
    "    test_labels.extend(labels)\n",
    "\n",
    "    print(\"Progress: {:.2f}%\".format((i+1)/len(ita_test_files)*100), end=\"\\r\")\n",
    "    print(\"Processed: {}/{}\".format(i+1, len(ita_test_files)), end=\"\\r\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data as pickle\n",
    "df = pd.DataFrame({'contour': test_chunks, 'labels': test_labels})\n",
    "df.to_pickle('ita_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12524\n"
     ]
    }
   ],
   "source": [
    "print(len(test_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.01659550862520646, 0.01659550862520646, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.20705475938828283, 0.20705475938828283, 0.1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0853309212266154, 0.0853309212266154, 0.085...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.017483717247693344, 0.017483717247693344, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.015928296040650866, 0.015928296040650866, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chunks  labels\n",
       "0  [0.01659550862520646, 0.01659550862520646, 0.0...       1\n",
       "1  [0.20705475938828283, 0.20705475938828283, 0.1...       1\n",
       "2  [0.0853309212266154, 0.0853309212266154, 0.085...       1\n",
       "3  [0.017483717247693344, 0.017483717247693344, 0...       1\n",
       "4  [0.015928296040650866, 0.015928296040650866, 0...       1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe\n",
    "df = pd.DataFrame({'contour': all_chunks, 'labels': all_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df.to_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe\n",
    "df2 = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.013564021110304844, 0.013564021110304844, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.4396198379683321, 0.4396198379683321, 0.280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.08664379438552769, 0.08664379438552769, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.05106373959591337, 0.05106373959591337, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.011421340988226685, 0.011421340988226685, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              chunks\n",
       "0  [0.013564021110304844, 0.013564021110304844, 0...\n",
       "1  [0.4396198379683321, 0.4396198379683321, 0.280...\n",
       "2  [0.08664379438552769, 0.08664379438552769, 0.1...\n",
       "3  [0.05106373959591337, 0.05106373959591337, 0.0...\n",
       "4  [0.011421340988226685, 0.011421340988226685, 0..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'contour': test_chunks})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94f4693ad758032fd28f9ecc08daa4776caeeaf7af79843cea3e1525fe817927"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
